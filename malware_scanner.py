#!/usr/bin/env python3
"""
Polyfill.io Scanner
A specialized tool for scanning websites specifically for polyfill.io references.
Scans through all links, scripts, and resources to identify polyfill.io usage.
"""

import requests
import re
import urllib.parse
import argparse
import json
import time
from typing import Set, List, Dict, Tuple
from urllib.robotparser import RobotFileParser
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from dataclasses import dataclass
from datetime import datetime

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('polyfill_scan.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ScanResult:
    """Data class to store scan results"""
    url: str
    threat_type: str
    description: str
    severity: str
    details: str = ""
    timestamp: str = ""

class PolyfillScanner:
    """Polyfill.io detection scanner class"""
    
    def __init__(self, max_workers: int = 5, delay: float = 1.0, bypass_robots: bool = False):
        self.max_workers = max_workers
        self.delay = delay
        self.bypass_robots = bypass_robots
        self.visited_urls: Set[str] = set()
        self.scan_results: List[ScanResult] = []
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Polyfill.io detection patterns only
        self.threat_patterns = {
            'polyfill.io': {
                'pattern': r'polyfill\.io',
                'description': 'Polyfill.io CDN - potentially compromised',
                'severity': 'HIGH'
            }
        }
        
        # Known polyfill.io domains
        self.malicious_domains = {
            'polyfill.io',
            'polyfill.com',
            'polyfill.net',
            'polyfill.org'
        }
        

    def is_valid_url(self, url: str) -> bool:
        """Check if URL is valid and should be scanned"""
        try:
            parsed = urllib.parse.urlparse(url)
            return parsed.scheme in ['http', 'https'] and parsed.netloc
        except:
            return False

    def normalize_url(self, url: str, base_url: str = None) -> str:
        """Normalize URL for consistent processing"""
        try:
            if base_url and not url.startswith(('http://', 'https://')):
                url = urllib.parse.urljoin(base_url, url)
            
            parsed = urllib.parse.urlparse(url)
            # Remove fragment and normalize
            normalized = urllib.parse.urlunparse((
                parsed.scheme, parsed.netloc, parsed.path,
                parsed.params, parsed.query, ''
            ))
            return normalized
        except:
            return url

    def can_fetch(self, url: str) -> bool:
        """Check robots.txt to see if we can fetch the URL"""
        if self.bypass_robots:
            logger.debug(f"Bypassing robots.txt check for {url}")
            return True
        
        try:
            parsed = urllib.parse.urlparse(url)
            robots_url = f"{parsed.scheme}://{parsed.netloc}/robots.txt"
            rp = RobotFileParser()
            rp.set_url(robots_url)
            rp.read()
            can_fetch = rp.can_fetch('*', url)
            if not can_fetch:
                logger.debug(f"robots.txt blocks access to {url}")
            return can_fetch
        except:
            return True  # If we can't check robots.txt, assume we can fetch

    def extract_links(self, content: str, base_url: str) -> Set[str]:
        """Extract all links from HTML content"""
        links = set()
        
        # Extract href attributes
        href_pattern = r'href=["\']([^"\']+)["\']'
        for match in re.finditer(href_pattern, content, re.IGNORECASE):
            link = self.normalize_url(match.group(1), base_url)
            if self.is_valid_url(link):
                links.add(link)
        
        # Extract src attributes
        src_pattern = r'src=["\']([^"\']+)["\']'
        for match in re.finditer(src_pattern, content, re.IGNORECASE):
            link = self.normalize_url(match.group(1), base_url)
            if self.is_valid_url(link):
                links.add(link)
        
        # Extract action attributes (forms)
        action_pattern = r'action=["\']([^"\']+)["\']'
        for match in re.finditer(action_pattern, content, re.IGNORECASE):
            link = self.normalize_url(match.group(1), base_url)
            if self.is_valid_url(link):
                links.add(link)
        
        # Extract URLs from JavaScript
        js_url_pattern = r'["\'](https?://[^"\']+)["\']'
        for match in re.finditer(js_url_pattern, content, re.IGNORECASE):
            link = match.group(1)
            if self.is_valid_url(link):
                links.add(link)
        
        return links

    def scan_content(self, content: str, url: str) -> List[ScanResult]:
        """Scan content for malware patterns"""
        results = []
        
        for threat_name, threat_info in self.threat_patterns.items():
            pattern = threat_info['pattern']
            matches = re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE)
            
            for match in matches:
                result = ScanResult(
                    url=url,
                    threat_type=threat_name,
                    description=threat_info['description'],
                    severity=threat_info['severity'],
                    details=f"Found at position {match.start()}: {match.group(0)[:100]}",
                    timestamp=datetime.now().isoformat()
                )
                results.append(result)
        
        # Check for malicious domains
        for domain in self.malicious_domains:
            if domain in content.lower():
                result = ScanResult(
                    url=url,
                    threat_type='malicious_domain',
                    description=f'Reference to known malicious domain: {domain}',
                    severity='HIGH',
                    details=f'Domain "{domain}" found in content',
                    timestamp=datetime.now().isoformat()
                )
                results.append(result)
        
        return results

    def fetch_url(self, url: str) -> Tuple[str, str, int]:
        """Fetch URL content and return content, content_type, and status_code"""
        try:
            response = self.session.get(url, timeout=10, allow_redirects=True)
            content_type = response.headers.get('content-type', '').lower()
            
            # Only process HTML and JavaScript content
            if any(ct in content_type for ct in ['text/html', 'text/javascript', 'application/javascript', 'application/x-javascript']):
                return response.text, content_type, response.status_code
            else:
                return '', content_type, response.status_code
                
        except requests.RequestException as e:
            logger.warning(f"Failed to fetch {url}: {e}")
            return '', '', 0

    def scan_url(self, url: str, max_depth: int = 3, current_depth: int = 0) -> List[ScanResult]:
        """Scan a single URL and its discovered links"""
        if url in self.visited_urls or current_depth >= max_depth:
            return []
        
        if not self.can_fetch(url):
            if self.bypass_robots:
                logger.debug(f"Bypassing robots.txt restrictions for {url}")
            else:
                logger.info(f"Skipping {url} due to robots.txt restrictions")
                return []
        
        self.visited_urls.add(url)
        logger.info(f"Scanning {url} (depth {current_depth})")
        
        # Add delay to be respectful
        time.sleep(self.delay)
        
        results = []
        content, content_type, status_code = self.fetch_url(url)
        
        if not content:
            return results
        
        # Scan the content for threats
        scan_results = self.scan_content(content, url)
        results.extend(scan_results)
        
        # Extract and scan discovered links
        if 'text/html' in content_type and current_depth < max_depth - 1:
            links = self.extract_links(content, url)
            
            # Limit the number of links to scan to prevent infinite crawling
            links_to_scan = list(links)[:20]  # Limit to first 20 links
            
            with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                future_to_url = {
                    executor.submit(self.scan_url, link, max_depth, current_depth + 1): link 
                    for link in links_to_scan
                }
                
                for future in as_completed(future_to_url):
                    link = future_to_url[future]
                    try:
                        link_results = future.result()
                        results.extend(link_results)
                    except Exception as e:
                        logger.error(f"Error scanning {link}: {e}")
        
        return results

    def scan_website(self, start_url: str, max_depth: int = 2) -> List[ScanResult]:
        """Main method to scan a website"""
        logger.info(f"Starting polyfill.io scan of {start_url}")
        
        if not self.is_valid_url(start_url):
            raise ValueError(f"Invalid URL: {start_url}")
        
        results = self.scan_url(start_url, max_depth)
        self.scan_results = results
        
        logger.info(f"Scan completed. Found {len(results)} polyfill.io references.")
        return results

    def generate_report(self, output_file: str = None) -> str:
        """Generate a detailed scan report"""
        if not self.scan_results:
            return "No polyfill.io references detected."
        
        # Group results by severity
        by_severity = {}
        for result in self.scan_results:
            severity = result.severity
            if severity not in by_severity:
                by_severity[severity] = []
            by_severity[severity].append(result)
        
        report = []
        report.append("=" * 80)
        report.append("POLYFILL.IO SCAN REPORT")
        report.append("=" * 80)
        report.append(f"Scan Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Total URLs Scanned: {len(self.visited_urls)}")
        report.append(f"Total Polyfill.io References Found: {len(self.scan_results)}")
        report.append("")
        
        # Summary by severity
        report.append("POLYFILL.IO REFERENCES BY SEVERITY:")
        report.append("-" * 40)
        for severity in ['HIGH', 'MEDIUM', 'LOW']:
            if severity in by_severity:
                count = len(by_severity[severity])
                report.append(f"{severity}: {count} references")
        report.append("")
        
        # Detailed results
        for severity in ['HIGH', 'MEDIUM', 'LOW']:
            if severity in by_severity:
                report.append(f"{severity} SEVERITY POLYFILL.IO REFERENCES:")
                report.append("-" * 40)
                
                for result in by_severity[severity]:
                    report.append(f"URL: {result.url}")
                    report.append(f"Type: {result.threat_type}")
                    report.append(f"Description: {result.description}")
                    report.append(f"Details: {result.details}")
                    report.append(f"Timestamp: {result.timestamp}")
                    report.append("")
        
        report_text = "\n".join(report)
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report_text)
            logger.info(f"Report saved to {output_file}")
        
        return report_text

    def export_json(self, output_file: str):
        """Export scan results to JSON format"""
        results_data = []
        for result in self.scan_results:
            results_data.append({
                'url': result.url,
                'threat_type': result.threat_type,
                'description': result.description,
                'severity': result.severity,
                'details': result.details,
                'timestamp': result.timestamp
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"JSON results exported to {output_file}")

def main():
    """Main function with command-line interface"""
    parser = argparse.ArgumentParser(description='Polyfill.io Scanner')
    parser.add_argument('url', help='URL to scan')
    parser.add_argument('--depth', type=int, default=2, help='Maximum crawl depth (default: 2)')
    parser.add_argument('--workers', type=int, default=5, help='Number of worker threads (default: 5)')
    parser.add_argument('--delay', type=float, default=1.0, help='Delay between requests in seconds (default: 1.0)')
    parser.add_argument('--bypass-robots', action='store_true', help='Bypass robots.txt restrictions (use with caution)')
    parser.add_argument('--output', help='Output file for report')
    parser.add_argument('--json', help='Export results to JSON file')
    parser.add_argument('--verbose', '-v', action='store_true', help='Verbose output')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        scanner = PolyfillScanner(max_workers=args.workers, delay=args.delay, bypass_robots=args.bypass_robots)
        if args.bypass_robots:
            logger.warning("WARNING: Bypassing robots.txt restrictions. Use responsibly!")
        results = scanner.scan_website(args.url, max_depth=args.depth)
        
        # Generate and display report
        report = scanner.generate_report(args.output)
        print(report)
        
        # Export to JSON if requested
        if args.json:
            scanner.export_json(args.json)
        
        # Exit with error code if polyfill.io references found
        polyfill_count = len([r for r in results if r.threat_type == 'polyfill.io'])
        if polyfill_count > 0:
            logger.warning(f"Found {polyfill_count} polyfill.io references!")
            exit(1)
        else:
            logger.info("No polyfill.io references found.")
            exit(0)
            
    except KeyboardInterrupt:
        logger.info("Scan interrupted by user")
        exit(130)
    except Exception as e:
        logger.error(f"Scan failed: {e}")
        exit(1)

if __name__ == "__main__":
    main()
